{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c4b5fd",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b61f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt # visualize satellite images\n",
    "from skimage.io import imshow # visualize satellite images\n",
    "\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout #Different components/layers of a neural network.\n",
    "from keras.models import Sequential # neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea279e",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc5f43",
   "metadata": {},
   "source": [
    "The provided data is encoded into CSV files, with X_test_sat4.csv containing flattened images extracted from space, measuring 28 x 28 x 4 in dimensions. \n",
    "\n",
    "These images consist of the conventional red, green, and blue channels seen in typical images, while the fourth channel represents near-infrared data. \n",
    "\n",
    "Due to the extensive size of the training set, I opted to utilize a smaller test set. \n",
    "\n",
    "Simultaneously, I am loading the second file which contains labels corresponding to each image. These labels fall into one of four categories: barren land, trees, grassland, and other. \n",
    "\n",
    "Each row within this file is structured as [1, 0, 0, 0], wherein only one of the four values is set to 1, indicating the respective class in the order presented earlier. For instance, if the label is [1, 0, 0, 0], the image depicts barren land. Similarly, [0, 1, 0, 0] signifies trees, [0, 0, 1, 0] denotes grassland, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa177e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data\n",
      "Loaded 28 x 28 x 4 images\n",
      "Loaded labels\n"
     ]
    }
   ],
   "source": [
    "x_train_path = 'X_test_sat4.csv' # Contains training images (28x28x4)\n",
    "y_train_path = 'y_test_sat4.csv' # Contains labels for training images (one-hot encoded)\n",
    "print ('Loading Training Data')\n",
    "X_train = pd.read_csv(x_train_path)\n",
    "print ('Loaded 28 x 28 x 4 images')\n",
    "\n",
    "Y_train = pd.read_csv(y_train_path)\n",
    "print ('Loaded labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d8b39",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4c1a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 99999 examples and each example is a list of 3136 numbers with 4 possible classifications.\n"
     ]
    }
   ],
   "source": [
    "# The loaded data is converted from pandas DataFrames to numpy arrays\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "print ('We have',X_train.shape[0],'examples and each example is a list of',\n",
    "       X_train.shape[1],'numbers with',Y_train.shape[1],'possible classifications.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068044af",
   "metadata": {},
   "source": [
    "## Reshaping Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3754c4",
   "metadata": {},
   "source": [
    "Upon extracting information from the CSV files, I can reshape the data to reconstruct the original images, enabling us to preview the images before commencing the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "896d159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999, 28, 28, 4)\n"
     ]
    }
   ],
   "source": [
    "'''The training images are reshaped from a list of numbers to a 4D tensor of shape (samples, width, height, channels):\n",
    "X_train is reshaped to X_train_img using .reshape() and cast to float.'''\n",
    "\n",
    "X_train_img = X_train.reshape([99999,28,28,4]).astype(float)\n",
    "print (X_train_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2063ac",
   "metadata": {},
   "source": [
    "# Visualising an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9c02317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barren Land\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at one image. Keep in mind the channels are R,G,B, and I(Infrared)\n",
    "\n",
    "'''An example image is visualized using skimage.io.imshow and matplotlib.pyplot.show():\n",
    "X_train_img[ix, :, :, 0:3] selects the RGB channels of the image.\n",
    "The corresponding label is displayed based on the one-hot encoded representation.'''\n",
    "\n",
    "ix = 5 #Type a number between 0 and 99,999 inclusive\n",
    "\n",
    "imshow(np.squeeze(X_train_img[ix,:,:,0:3] * 255).astype(np.uint8)) #Only seeing the RGB channels\n",
    "plt.show()\n",
    "\n",
    "#Tells what the image is\n",
    "\n",
    "if Y_train[ix,0] == 1:\n",
    "    print ('Barren Land')\n",
    "elif Y_train[ix,1] == 1:\n",
    "    print ('Trees')\n",
    "elif Y_train[ix,2] == 1:\n",
    "    print ('Grassland')\n",
    "else:\n",
    "    print ('Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e243de1",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af8cdb",
   "metadata": {},
   "source": [
    "We have the option to select between two distinct model types: a basic artificial neural network or a convolutional neural network. To begin with, I will opt for the simpler artificial neural network. This network will consist of only a single layer, which is the output layer. It's important to note that this particular network is not anticipated to possess significant computational power and is likely to exhibit relatively sluggish performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a9f5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A simple neural network model is defined using the Sequential API:\n",
    "The model consists of a single dense layer with 4 output units (one for each class) and a softmax activation function.'''\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(4, input_shape=(3136,), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af78f6d9",
   "metadata": {},
   "source": [
    "## Data Normalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89ff8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data (X_train_img) is normalized by dividing all values by 255.\n",
    "X_train = X_train/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bea98",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd08fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 12548     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,548\n",
      "Trainable params: 12,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "2813/2813 [==============================] - 4s 2ms/step - loss: 0.6474 - accuracy: 0.7283 - val_loss: 0.6326 - val_accuracy: 0.7243\n",
      "Epoch 2/5\n",
      "2813/2813 [==============================] - 4s 1ms/step - loss: 0.6382 - accuracy: 0.7301 - val_loss: 0.5957 - val_accuracy: 0.7457\n",
      "Epoch 3/5\n",
      "2813/2813 [==============================] - 4s 1ms/step - loss: 0.6358 - accuracy: 0.7333 - val_loss: 0.5748 - val_accuracy: 0.7571\n",
      "Epoch 4/5\n",
      "2813/2813 [==============================] - 4s 1ms/step - loss: 0.6306 - accuracy: 0.7340 - val_loss: 0.5735 - val_accuracy: 0.7593\n",
      "Epoch 5/5\n",
      "2813/2813 [==============================] - 4s 1ms/step - loss: 0.6327 - accuracy: 0.7346 - val_loss: 0.5892 - val_accuracy: 0.7532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1065d4c70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Optimizer: Adam optimizer.\n",
    "Loss function: Categorical cross-entropy (suitable for multiclass classification).\n",
    "Metrics: Accuracy'''\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train,Y_train,batch_size=32, epochs=5, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed805a",
   "metadata": {},
   "source": [
    "## Evaluating the model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73344f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3094/3094 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Selects the last 1000 images\n",
    "\n",
    "preds = model.predict(X_train[1000:], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3588b435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "100.0% probability barren land,\n",
      "0.0% probability trees,\n",
      "0.0% probability grassland,\n",
      "0.0% probability other\n",
      "\n",
      "Ground Truth: Barren Land\n"
     ]
    }
   ],
   "source": [
    "ix = 90 # Type a number between 0 and 999 inclusive\n",
    "imshow(np.squeeze(X_train_img[ix,:,:,0:3] * 255).astype(np.uint8)) #Only seeing the RGB channels\n",
    "plt.show()\n",
    "# Tells what the image is\n",
    "print ('Prediction:\\n{:.1f}% probability barren land,\\n{:.1f}% probability trees,\\n{:.1f}% probability grassland,\\n{:.1f}% probability other\\n'.format(preds[ix,0]*100,preds[ix,1]*100,preds[ix,2]*100,preds[ix,3]*100))\n",
    "\n",
    "print ('Ground Truth: ',end='')\n",
    "if Y_train[99999-(1000-ix),0] == 1:\n",
    "    print ('Barren Land')\n",
    "elif Y_train[99999-(1000-ix),1] == 1:\n",
    "    print ('Trees')\n",
    "elif Y_train[99999-(1000-ix),2] == 1:\n",
    "    print ('Grassland')\n",
    "else:\n",
    "    print ('Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142bf20",
   "metadata": {},
   "source": [
    "# Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f9d06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imshow\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a248534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data\n",
      "Loaded 28 x 28 x 4 images\n",
      "Loaded labels\n",
      "We have 99999 examples and each example is a list of 3136 numbers with 4 possible classifications.\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "x_train_path = 'X_test_sat4.csv'\n",
    "y_train_path = 'y_test_sat4.csv'\n",
    "print('Loading Training Data')\n",
    "X_train = pd.read_csv(x_train_path)\n",
    "print('Loaded 28 x 28 x 4 images')\n",
    "\n",
    "Y_train = pd.read_csv(y_train_path)\n",
    "print('Loaded labels')\n",
    "\n",
    "# Data Preprocessing\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "print('We have', X_train.shape[0], 'examples and each example is a list of',\n",
    "      X_train.shape[1], 'numbers with', Y_train.shape[1], 'possible classifications.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb04a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 4) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Evaluating the model on images\u001b[39;00m\n\u001b[1;32m     25\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/16/n85snlw97_g_bmzt1m93tm8w0000gn/T/__autograph_generated_file25nug1_w.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/palukgupta/opt/anaconda3/lib/python3.9/site-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 4) are incompatible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_img = X_train.reshape(-1, 28, 28, 4)\n",
    "Y_train = Y_train.argmax(axis=1)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_img, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Defining the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 4)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_val, Y_val))\n",
    "\n",
    "\n",
    "# Evaluating the model on images\n",
    "ix = 90\n",
    "imshow(np.squeeze(X_val[ix, :, :, 0:3]).astype(np.uint8))\n",
    "plt.show()\n",
    "\n",
    "preds = model.predict(X_val[ix:ix + 1], verbose=1)\n",
    "\n",
    "print('Prediction:\\n{:.1f}% probability barren land,\\n{:.1f}% probability trees,\\n{:.1f}% probability grassland,\\n{:.1f}% probability other\\n'.format(preds[0, 0] * 100, preds[0, 1] * 100, preds[0, 2] * 100, preds[0, 3] * 100))\n",
    "\n",
    "ground_truth = np.argmax(Y_val[ix], axis=0)\n",
    "if ground_truth == 0:\n",
    "    print('Barren Land')\n",
    "elif ground_truth == 1:\n",
    "    print('Trees')\n",
    "elif ground_truth == 2:\n",
    "    print('Grassland')\n",
    "else:\n",
    "    print('Other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ca6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
